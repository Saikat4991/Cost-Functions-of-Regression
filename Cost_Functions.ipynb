{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTXLtAlyq614UYyvUwoaXR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saikat4991/Cost-Functions-of-Regression/blob/main/Cost_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is cost funtion?\n",
        "\n",
        "A cost function in the context of machine learning and specifically linear regression, is a mathematical formula that measures the difference between the *predicted values by the model* and *the actual values from the data*.\n",
        "\n",
        "- It quantifies the error of the model as a single numeric value.\n",
        "- The goal in training the model is to find the parameters (weights) that minimize this error, indicating the model's predictions are as close as possible to the real outcomes."
      ],
      "metadata": {
        "id": "2n2aCxGDRq1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why do we use Cost Function?\n",
        "\n",
        "The cost function in linear regression is crucial for:\n",
        "\n",
        "1. **Measuring Accuracy**: It quantifies the model's error, allowing for clear evaluation.\n",
        "2. **Guiding Optimization**: Directs model improvement by minimizing error.\n",
        "3. **Efficiency**: Simple and computationally efficient for optimization.\n",
        "4. **Model Comparison**: Enables objective comparison between different models.\n",
        "5. **Learning Mechanism**: Encourages patterns learning by penalizing errors.\n",
        "6. **Versatility**: Adaptable across various machine learning models and problems.\n",
        "\n",
        "Its role is central to optimizing and evaluating predictive models."
      ],
      "metadata": {
        "id": "AS0crpd0SLgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What are the different cost functions in linear regression?\n",
        "\n",
        "In linear regression, several cost functions can be used to measure the model's performance by quantifying the error between the predicted values and the actual values. The choice of cost function can depend on the specific characteristics of the data or the objectives of the regression analysis. The most common cost functions in linear regression include:\n",
        "\n",
        "1. **Mean Squared Error (MSE)**: This is the most commonly used cost function in linear regression. It calculates the average of the squared differences between the predicted and actual values. It is sensitive to outliers because it squares the errors before averaging them.\n",
        "\n",
        "   $ MSE = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 $\n",
        "\n",
        "2. **Root Mean Squared Error (RMSE)**: RMSE is the square root of MSE, which scales the errors back to their original units. Like MSE, it is sensitive to outliers but is more interpretable because it is in the same units as the response variable.\n",
        "\n",
        "   $ RMSE = \\sqrt{\\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2} $\n",
        "\n",
        "3. **Mean Absolute Error (MAE)**: MAE measures the average of the absolute differences between the predicted values and the actual values. It is less sensitive to outliers than MSE and RMSE, making it a good choice when the data contains anomalies.\n",
        "\n",
        "   $ MAE = \\frac{1}{m} \\sum_{i=1}^{m} |y_i - \\hat{y}_i| $\n",
        "\n"
      ],
      "metadata": {
        "id": "KKXb1hZ2Sz68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mean Squared Error (MSE)\n",
        "\n",
        "The ***Mean Squared Error (MSE)*** is a widely used cost function in linear regression and other predictive modeling techniques. It calculates the average of the squares of the errors, i.e., the differences between the predicted and actual values.\n",
        "\n",
        "### Advantages\n",
        "\n",
        "1. **Simple Interpretation**: MSE directly corresponds to the average squared difference between the estimated values and the actual value, making it easy to interpret and understand.\n",
        "2. **Differentiability**: MSE is a smooth and continuous function, which is beneficial for optimization using gradient descent or other algorithms, as it allows for the calculation of gradients.\n",
        "3. **Sensitivity to Outliers**: Due to squaring the differences, MSE is very sensitive to outliers. This can be an advantage in scenarios where it is important to not overlook large errors.\n",
        "4. **Consistency**: MSE provides a consistent measure of performance across different datasets or models, making comparison straightforward.\n",
        "\n",
        "### Disadvantages\n",
        "\n",
        "1. **Sensitivity to Outliers**: The same property that can be an advantage becomes a disadvantage when dealing with noisy data that contains many outliers. The squaring term can lead to large error values for outliers, heavily influencing the model performance and potentially leading to overfitting to outliers.\n",
        "2. **Scale Dependence**: MSE values are dependent on the scale of the data, making it difficult to compare model performance across datasets with different scales or units.\n",
        "3. **Not Intuitive Units**: The units of MSE are squared units of the output variable, which can be less intuitive to interpret compared to other metrics like Mean Absolute Error (MAE), which is in the same units as the output variable.\n",
        "4. **Impact on Gradient Descent**: The sensitivity to outliers can also impact the optimization process, with gradients potentially dominated by large errors, which might lead to suboptimal updates.\n",
        "\n",
        "It works best in scenarios where the data is free of outliers, and the model's performance is not being compared across datasets with vastly different scales."
      ],
      "metadata": {
        "id": "D5NOqzBmUJ-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Root Mean Squared Error (RMSE)\n",
        "\n",
        "The ***Root Mean Squared Error (RMSE)*** is a standard way to measure the error of a model in predicting quantitative data. It is essentially the square root of the Mean Squared Error (MSE). RMSE offers a way to convert the error metric back into the same units as the original data, which can provide more interpretable insights.\n",
        "\n",
        "### Advantages\n",
        "\n",
        "1. **Unit Consistency**: RMSE is in the same units as the output variable, making it more intuitive and easier to interpret than MSE, especially when communicating results to non-technical stakeholders.\n",
        "2. **Sensitivity to Outliers**: Like MSE, RMSE is sensitive to outliers, emphasizing large errors more than smaller ones, which can be useful in applications where large errors are particularly undesirable.\n",
        "3. **Differentiability**: RMSE maintains the differentiable property of MSE, allowing for the use of gradient-based optimization methods which are essential for training models.\n",
        "4. **Scale Interpretation**: Provides a clear scale of error and can be directly related to the data, helping in understanding the magnitude of prediction errors.\n",
        "\n",
        "### Disadvantages\n",
        "\n",
        "1. **Sensitivity to Outliers**: The sensitivity to outliers, while sometimes advantageous, can also be a drawback. Outliers can disproportionately affect the RMSE, leading to a skewed perception of model performance, especially in datasets with significant noise or outliers.\n",
        "2. **Not Average Error**: RMSE does not directly give an average error, which can sometimes make it less straightforward to gauge the model's performance on an individual prediction level.\n",
        "3. **Misleading with Small Datasets**: In smaller datasets, the RMSE can be particularly sensitive to the sampling of the data, potentially leading to overestimations or underestimations of model performance.\n",
        "4. **Impact on Model Selection**: The heavy penalty on large errors might lead to model selection or tuning that overly prioritizes minimizing large errors at the expense of overall prediction accuracy across all data points.\n",
        "\n",
        "In essence, RMSE is beneficial for its interpretability and emphasis on large errors, making it a valuable metric in many predictive modeling tasks. However, its sensitivity to outliers and the potential for misinterpretation necessitate careful consideration, especially in datasets known for noise and outliers."
      ],
      "metadata": {
        "id": "I2yd-Tc7UViW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mean Absolute Error (MAE)\n",
        "\n",
        "The ***Mean Absolute Error (MAE)*** is another common cost function used in linear regression and various other predictive modeling techniques. It calculates the average of the absolute differences between the predicted values and the actual observed values. Here are the advantages and disadvantages of using MAE:\n",
        "\n",
        "### Advantages\n",
        "\n",
        "1. **Intuitive Interpretation**: MAE measures the average magnitude of errors in a set of predictions, without considering their direction. The units of MAE are the same as those of the output variable, making it straightforward and intuitive to interpret.\n",
        "2. **Unit Consistency**: MAE is in the same units as the output variable, making it more intuitive and easier to interpret than MSE, especially when communicating results to non-technical stakeholders.\n",
        "3. **Robustness to Outliers**: Unlike MSE, MAE is less sensitive to outliers because it does not square the error terms. This makes it more robust in datasets where outliers are present but should not heavily influence the model's performance.\n",
        "4. **Linear Penalty**: Each error contributes proportionally to the total amount of error, giving a linear view of the total error. This can be more desirable when large errors are not necessarily exponentially worse than smaller ones.\n",
        "\n",
        "### Disadvantages\n",
        "\n",
        "1. **Non-Differentiability at Zero**: The absolute value function is not differentiable at zero, which can pose challenges for certain optimization algorithms that rely on differentiation, such as gradient descent. However, in practice, this issue can often be managed.\n",
        "2. **Equal Weighting to All Errors**: While robustness to outliers is an advantage, MAE gives equal weighting to all errors. This can be a disadvantage if the goal is to more heavily penalize larger errors, which MSE naturally does.\n",
        "3. **Less Sensitive to Precise Predictions**: Because MAE treats all errors equally, it might not sufficiently reward a model for making very close predictions as compared to slightly off predictions, unlike MSE, which magnifies the rewards for closer predictions due to the squaring of errors.\n",
        "4. **Optimization Challenges**: For some models, optimizing for MAE directly can be more challenging than optimizing for MSE due to the nature of its gradient, which can lead to slower convergence in some cases.\n",
        "\n",
        "In summary, MAE is a useful metric for evaluating model performance, especially in the presence of outliers or when a simple, linear measure of error is preferred. Its robustness and intuitive interpretation make it appealing for many practical applications, though considerations around its optimization characteristics and sensitivity to error magnitudes should be taken into account."
      ],
      "metadata": {
        "id": "SwZ9gsHpUaPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom functions for MSE, MAE, and RMSE\n",
        "\n",
        "def custom_mse(y_true, y_pred):\n",
        "    \"\"\"Calculate Mean Squared Error (MSE)\"\"\"\n",
        "    mse = np.mean((y_true - y_pred) ** 2)\n",
        "    return mse\n",
        "\n",
        "def custom_mae(y_true, y_pred):\n",
        "    \"\"\"Calculate Mean Absolute Error (MAE)\"\"\"\n",
        "    mae = np.mean(np.abs(y_true - y_pred))\n",
        "    return mae\n",
        "\n",
        "def custom_rmse(y_true, y_pred):\n",
        "    \"\"\"Calculate Root Mean Squared Error (RMSE)\"\"\"\n",
        "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
        "    return rmse"
      ],
      "metadata": {
        "id": "Raq89p73drs3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Step 1: Load the California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "y = pd.Series(housing.target)\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "0X_VEHjJWyjN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Calculate MSE, MAE, and RMSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)  # RMSE is the square root of MSE\n",
        "\n",
        "\n",
        "# Step 6: Use the custom functions to calculate MSE, MAE, and RMSE\n",
        "custom_mse_result = custom_mse(y_test, y_pred)\n",
        "custom_mae_result = custom_mae(y_test, y_pred)\n",
        "custom_rmse_result = custom_rmse(y_test, y_pred)\n",
        "\n",
        "# Step 7: Print the results from custom and sklearn functions\n",
        "print(\"Custom Mean Squared Error (MSE):\", custom_mse_result)\n",
        "print(\"Custom Mean Absolute Error (MAE):\", custom_mae_result)\n",
        "print(\"Custom Root Mean Squared Error (RMSE):\", custom_rmse_result)\n",
        "\n",
        "\n",
        "print(\"\\nsklearn Mean Squared Error (MSE):\", mse)\n",
        "print(\"sklearn Mean Absolute Error (MAE):\", mae)\n",
        "print(\"sklearn Root Mean Squared Error (RMSE):\", rmse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu8rb6EGXoPm",
        "outputId": "a43c5cef-4468-4fb3-9add-f6b4154ff6fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Mean Squared Error (MSE): 0.5558915986952444\n",
            "Custom Mean Absolute Error (MAE): 0.5332001304956553\n",
            "Custom Root Mean Squared Error (RMSE): 0.7455813830127764\n",
            "\n",
            "sklearn Mean Squared Error (MSE): 0.5558915986952444\n",
            "sklearn Mean Absolute Error (MAE): 0.5332001304956553\n",
            "sklearn Root Mean Squared Error (RMSE): 0.7455813830127764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1gtKfNi0bzwv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}